# -*- coding: utf-8 -*-
"""Copy of DL_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VAtFSe2x8sQgnkOCLrONT6dU1vNNd5_h
"""

import pandas as pd
import numpy as np
from tqdm import tqdm
import os
import sys
import librosa
import librosa.display
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from IPython.display import Audio

import tensorflow as tf
import keras
from keras.callbacks import ReduceLROnPlateau
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization,Input,Conv2D,MaxPooling2D, InputLayer, UpSampling2D, Layer, Reshape
from keras.utils import np_utils, to_categorical
from keras.callbacks import ModelCheckpoint

import warnings
if not sys.warnoptions:
    warnings.simplefilter("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)

!unzip "/content/drive/MyDrive/archive.zip" -d "ravdess"

import shutil
shutil.rmtree('/content/ravdess/audio_speech_actors_01-24', ignore_errors=True)

Ravdess="/content/ravdess"
ravdess_directory_list = os.listdir("/content/ravdess")

file_gender=[]
file_emotion = []
file_path = []
for dir in ravdess_directory_list:
    '''# as their are 20 different actors in our previous directory we need to extract files for each actor.'''
    directory=os.path.join(Ravdess, dir)
    actor = os.listdir(directory)
    for i,file in enumerate(actor):
        part = file.split('.')[0]
        part = part.split('-')
        
        '''# third part in each file represents the emotion associated to that file.'''
        file_emotion.append(int(part[2]))
        temp = int(part[6])
        if temp%2 == 0:
            temp = "female"
        else:
            temp = "male"
        file_gender.append(temp)
        file_path.append(Ravdess +'/'+ dir + '/' + file)
emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])
emotion_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)
gender=pd.DataFrame(file_gender,columns=['Gender'])
emotion_df=pd.concat([emotion_df,gender],axis=1)

'''# dataframe for path of files.'''
path_df = pd.DataFrame(file_path, columns=['Path'])
Ravdess_df = pd.concat([emotion_df, path_df], axis=1)
Ravdess_df['labels'] =Ravdess_df.Gender + '_' + Ravdess_df.Emotions
Ravdess_df=Ravdess_df.drop('Emotions',axis=1)
Ravdess_df=Ravdess_df.drop('Gender',axis=1)

'''# changing integers to actual emotions.'''

Ravdess_df.head()

Ravdess_df['labels'].value_counts()

def create_waveplot(data, sr, e):
    plt.figure(figsize=(10, 3))
    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)
    librosa.display.waveshow(data, sr=sr)
    
    plt.show()

def create_spectrogram(data, sr, e):

    '''# stft function converts the data into short term fourier transform'''
    X = librosa.stft(data)
    Xdb = librosa.amplitude_to_db(abs(X))
    plt.figure(figsize=(12, 3))
    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)
    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   
  
    plt.colorbar()

emotion='female_angry'
path = np.array(Ravdess_df.Path[Ravdess_df.labels==emotion])[1]
print(path)
data, sampling_rate = librosa.load(path)
create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

emotion='male_angry'
path = np.array(Ravdess_df.Path[Ravdess_df.labels==emotion])[1]
print(path)
data, sampling_rate = librosa.load(path)
create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

def noise(data):
    noise_amp = 0.035*np.random.uniform()*np.amax(data)
    data = data + noise_amp*np.random.normal(size=data.shape[0])
    return data

def stretch(data, rate=0.8):
    return librosa.effects.time_stretch(data,rate= 0.8)

def shift(data):
    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)
    return np.roll(data, shift_range)

def pitch(data, sampling_rate, pitch_factor=0.7):
    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)

'''# taking any example and checking for techniques.'''

path = np.array(Ravdess_df.Path)[1]
data, sample_rate = librosa.load(path)

def extract_features(data):
    '''# ZCR'''
    result = np.array([])
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)
    result=np.hstack((result, zcr)) 

    '''# Chroma_stft'''
    stft = np.abs(librosa.stft(data))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)
    result = np.hstack((result, chroma_stft)) 
    '''# MFCC'''
    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)
    result = np.hstack((result, mfcc)) 

    '''# Root Mean Square Value'''
    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)
    result = np.hstack((result, rms)) 

    '''# MelSpectogram'''
    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)
    result = np.hstack((result, mel)) 
    
    return result
def get_features(path):

    '''# duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.'''
    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)
    
    '''# without augmentation'''
    res1 = extract_features(data)
    result = np.array(res1)
    
    '''# data with noise'''
    noise_data = noise(data)
    res2 = extract_features(noise_data)
    result = np.vstack((result, res2)) 
    
    '''# data with stretching and pitching'''
    
    res3 = extract_features(data)
    result = np.vstack((result, res3)) 
    
    return result

X, Y = [], []
for path, emotion in tqdm(zip(Ravdess_df.Path, Ravdess_df.labels)):
    feature = get_features(path)
    for ele in feature:
        X.append(ele)
        
        Y.append(emotion)

len(X), len(Y), Ravdess_df.Path.shape

Features = pd.DataFrame(X)
Features['labels'] = Y
Features.to_csv('features.csv', index=False)
Features.head()

X = Features.iloc[: ,:-1].values
Y = Features['labels'].values

encoder = OneHotEncoder()
Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()

x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

'''# making our data compatible to model.'''

x_train = np.expand_dims(x_train, axis=2)
x_test = np.expand_dims(x_test, axis=2)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

"""***CNN***"""

model=Sequential()
model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
model.add(Dropout(0.2))

model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
model.add(Flatten())
model.add(Dense(units=32, activation='relu'))
model.add(Dropout(0.3))

model.add(Dense(units=16, activation='softmax'))
model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])

rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)
history=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])

'''# Save model and weights'''

model_name = 'Emotion_Model.h5'
save_dir = os.path.join(os.getcwd(), 'saved_models')

if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
print('Save model and weights at %s ' % model_path)

'''# Save the model to disk'''

model_json = model.to_json()
with open("model_json.json", "w") as json_file:
    json_file.write(model_json)

print("Accuracy of our model on test data : " , model.evaluate(x_test,y_test)[1]*100 , "%")

epochs = [i for i in range(50)]
fig , ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
test_acc = history.history['val_accuracy']
test_loss = history.history['val_loss']

fig.set_size_inches(20,6)
ax[0].plot(epochs , train_loss , label = 'Training Loss')
ax[0].plot(epochs , test_loss , label = 'Testing Loss')
ax[0].set_title('Training & Testing Loss')
ax[0].legend()
ax[0].set_xlabel("Epochs")
ax[1].plot(epochs , train_acc , label = 'Training Accuracy')
ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')
ax[1].set_title('Training & Testing Accuracy')
ax[1].legend()
ax[1].set_xlabel("Epochs")
plt.show()

'''# predicting on test data.'''

pred_test = model.predict(x_test)
y_pred = encoder.inverse_transform(pred_test)

y_test = encoder.inverse_transform(y_test)

df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])
df['Predicted Labels'] = y_pred.flatten()
df['Actual Labels'] = y_test.flatten()

df.head(10)

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize = (12, 10))
cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])
sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')
plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()

print(classification_report(y_test, y_pred))

"""Self-supervised"""

pip install transformers

pip install datasets

label_map = {'female_angry': 0, 'female_calm': 1, 'female_disgust': 2, 'female_fear': 3,'female_happy':4,'female_neutral':5,'female_sad':6,'female_surprise':7,'male_angry':8,'male_calm':9,'male_disgust':10,'male_fear':11,'male_happy':12,'male_neutral':13,'male_sad':14,'male_surprise':15}
Ravdess_df['labels'] = Ravdess_df['labels'].map(label_map)

train_df = Ravdess_df.sample(frac=0.8)
test_df = Ravdess_df.drop(train_df.index)

from transformers import Wav2Vec2FeatureExtractor
from datasets import Dataset

def map_to_array(example):
    speech, _ = librosa.load(example["Path"], sr=16000, mono=True)
    example["speech"] = speech
    return example

train_data = Dataset.from_pandas(train_df).map(map_to_array)
test_data = Dataset.from_pandas(test_df).map(map_to_array)

feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("superb/hubert-large-superb-er")

train_encodings = feature_extractor(list(train_data["speech"]), sampling_rate=16000, padding=True, return_tensors="pt")
test_encodings = feature_extractor(list(test_data["speech"]), sampling_rate=16000, padding=True, return_tensors="pt")

print(train_data[1])

import torch

class EmotionDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        pattern = {0:0, 1:1, 2:2, 3:3, 4:4,5:5,6:6,7:7,8:8,9:9,10:10,11:11,12:12,13:13,14:14,15:15}
        self.labels = [pattern[x] for x in labels]

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)
    

train_dataset = EmotionDataset(train_encodings, list(train_data["labels"]))
test_dataset = EmotionDataset(test_encodings, list(test_data["labels"]))

from transformers import HubertForSequenceClassification,HubertConfig
from torch.optim import AdamW

'''# Loading the model'''
model = HubertForSequenceClassification.from_pretrained("facebook/hubert-large-ls960-ft")
config = model.config
config.num_labels = 16
new_model = HubertForSequenceClassification(config).cuda()

'''# Loading the optimizer'''
optim = AdamW(new_model.parameters(), lr=1e-3)

for param in new_model.parameters():
    param.requires_grad = False
      
layers_freeze_num = 2
n_layers = (
    4 + layers_freeze_num * 16)
for name, param in list(new_model.named_parameters())[-n_layers:]:
    param.requires_grad = True

'''# Prediction function'''

def predict(outputs):
    probabilities = torch.softmax(outputs["logits"], dim=1)
    
    predictions = torch.argmax(probabilities, dim=1)
    return predictions

'''# Training'''

from torch.utils.data import DataLoader

'''# Set the number of epoch'''
epoch = 50

'''# Start training'''

model.train()
train_loss = list()
train_accuracies = list()
for epoch_i in range(epoch):
    print('Epoch %s/%s' % (epoch_i + 1, epoch))
    
    '''# Get training data by DataLoader'''
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    
    correct = 0
    count = 0
    epoch_loss = list()
    
    pbar = tqdm(train_loader)
    for batch in pbar:
        optim.zero_grad()
        input_ids = batch['input_values'].cuda()
        attention_mask = batch['attention_mask'].cuda()
        labels = batch['labels'].cuda()
        
        outputs = new_model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs['loss']
        loss.backward()
        optim.step()
        
        '''# make predictions'''
        predictions = predict(outputs)
        
        '''# count accuracy'''
        correct += predictions.eq(labels).sum().item()
        count += len(labels)
        accuracy = correct * 1.0 / count
        
        '''# show progress along with metrics
        # record the loss for each batch'''

        epoch_loss.append(loss.item())
    '''# record the loss and accuracy for each epoch'''

    train_loss += epoch_loss
    print(accuracy)
    train_accuracies.append(accuracy)

print(train_accuracies)

"""CNN-Pytorch"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os, glob
import librosa
import librosa.display
import IPython
from IPython.display import Audio
from IPython.display import Image
import warnings; warnings.filterwarnings('ignore')

sample_rate = 48000
def feature_mfcc(
    waveform, 
    sample_rate,
    n_mfcc = 40,
    fft = 1024,
    winlen = 512,
    window='hamming',
    mels=128
    ):
  
    '''# Compute the MFCCs for all STFT frames 
    # 40 mel filterbanks (n_mfcc) = 40 coefficients'''

    mfc_coefficients=librosa.feature.mfcc(
        y=waveform, 
        sr=sample_rate, 
        n_mfcc=n_mfcc,
        n_fft=fft, 
        win_length=winlen, 
        window=window, 
        n_mels=mels, 
        fmax=sample_rate/2
        ) 
    
    return mfc_coefficients

def get_features(waveforms, features, samplerate):

    '''# initialize counter to track progress'''
    file_count = 0

    '''# process each waveform individually to get its MFCCs'''
    for waveform in waveforms:
        
        mfccs = feature_mfcc(waveform, sample_rate)
        
        features.append(mfccs)
        file_count += 1
        
        '''# print progress''' 

        print('\r'+f' Processed {file_count}/{len(waveforms)} waveforms',end='')
    
    '''# return all features from list of waveforms'''
    return features

def get_waveforms(file):
    
    '''# load an individual sample audio file
    # read the full 3 seconds of the file, cut off the first 0.5s of silence; native sample rate = 48k
    # don't need to store the sample rate that librosa.load returns'''
    
    waveform, _ = librosa.load(file, duration=3, offset=0.5, sr=sample_rate)
    waveform_homo = np.zeros((int(sample_rate*3,)))
    waveform_homo[:len(waveform)] = waveform
    
    '''# return a single file's waveform'''                                      
    return waveform_homo

emotions_dict ={
    '0':'surprised',
    '1':'neutral',
    '2':'calm',
    '3':'happy',
    '4':'sad',
    '5':'angry',
    '6':'fearful',
    '7':'disgust'
}

'''# Additional attributes from RAVDESS to play with'''

emotion_attributes = {
    '01': 'normal',
    '02': 'strong'
}

Ravdess="/content/ravdess"
ravdess_directory_list = os.listdir("/content/ravdess")
def load_data():
    '''# features and labels'''

    emotions = []

    '''# raw waveforms to augment later'''
    waveforms = []

    '''# extra labels'''
    intensities = []

    '''# progress counter'''
    file_count = 0

    for dir in ravdess_directory_list:
    '''# as their are 20 different actors in our previous directory we need to extract files for each actor.'''

      directory=os.path.join(Ravdess, dir)
      actor = os.listdir(directory)
      for file_name in actor:
            file_path=Ravdess +'/'+ dir + '/' + file_name
            
            '''# get file name with labels'''
            part = file_name.split('.')[0]
            
            '''# get emotion label from the sample's file'''
            emotion = int(file_name.split("-")[2])
            
            '''#  move surprise to 0 for cleaner behaviour with PyTorch/0-indexing'''
            if emotion == 8: emotion = 0 # surprise is now at 0 index; other emotion indeces unchanged

            '''# can convert emotion label to emotion string if desired, but
            # training on number is better; better convert to emotion string after predictions are ready'''
            emotion = emotions_dict[str(emotion)]
            
            '''# get other labels we might want'''
            intensity = emotion_attributes[file_name.split("-")[3]]
            
            '''# even actors are female, odd are male'''          
            if (int((file_name.split("-")[6]).split(".")[0]))%2==0: 
                gender = 'female' 
            else: 
                gender = 'male'
                
            '''# get waveform from the sample'''
            waveform = get_waveforms(file_path)
            
            '''# store waveforms and labels'''
            waveforms.append(waveform)
            emotions.append(emotion)
            intensities.append(intensity) # store intensity in case we wish to predict
            genders.append(gender) # store gender in case we wish to predict 
            
            file_count += 1
            
            '''# keep track of data loader's progress'''
            print('\r'+f' Processed {file_count}/{1440} audio samples',end='')
            
    return waveforms, emotions, intensities, genders

waveforms, emotions, intensities, genders = [],[],[],[]
waveforms, emotions, intensities, genders = load_data()

train_set,valid_set,test_set = [],[],[]
X_train,X_valid,X_test = [],[],[]
y_train,y_valid,y_test = [],[],[]

waveforms = np.array(waveforms)

print(emotions)

for i,emotion_value in enumerate(emotions_dict.values()):
    
    '''# find all indices of a single unique emotion'''
    
    emotion_indices = [index for index, emotion in enumerate(emotions) if str(emotion)==str(emotion_value)]
    
    '''# seed for reproducibility''' 
    np.random.seed(69)

    '''# shuffle indicies'''
    emotion_indices = np.random.permutation(emotion_indices)
    
    '''# store dim (length) of the emotion list to make indices'''
    dim = len(emotion_indices)

    '''# store indices of training, validation and test sets in 80/10/10 proportion
    # train set is first 80%'''
    train_indices = emotion_indices[:int(0.8*dim)]
    
    '''# validation set is next 10% (between 80% and 90%)'''
    valid_indices = emotion_indices[int(0.8*dim):int(0.9*dim)]
    
    '''# test set is last 10% (between 90% - end/100%)'''
    test_indices = emotion_indices[int(0.9*dim):]
    X_train.append(waveforms[train_indices,:])
    x=np.full((len(train_indices),), i,dtype=np.float32)
    y_train.append(x)

    '''# create validation waveforms/labels sets'''
    X_valid.append(waveforms[valid_indices,:])
    y=x=np.full((len(test_indices),), i,dtype=np.float32)
    y_valid.append(y)
    
    '''# create test waveforms/labels sets'''
    X_test.append(waveforms[test_indices,:])
    z=x=np.full((len(test_indices),), i,dtype=np.float32)
    y_test.append(z)

    '''# store indices for each emotion set to verify uniqueness between sets''' 
    train_set.append(train_indices)
    valid_set.append(valid_indices)
    test_set.append(test_indices)

X_train = np.concatenate(X_train,axis=0)
X_valid = np.concatenate(X_valid,axis=0)
X_test = np.concatenate(X_test,axis=0)

'''# concatenate, in order, all emotions back into one array'''
y_train = np.concatenate(y_train,axis=0)
y_valid = np.concatenate(y_valid,axis=0)
y_test = np.concatenate(y_test,axis=0)

train_set = np.concatenate(train_set,axis=0)
valid_set = np.concatenate(valid_set,axis=0)
test_set = np.concatenate(test_set,axis=0)

features_train, features_valid, features_test = [],[],[]

print('Train waveforms:') # get training set features 
features_train = get_features(X_train, features_train, sample_rate)

print('\n\nValidation waveforms:') # get validation set features
features_valid = get_features(X_valid, features_valid, sample_rate)

print('\n\nTest waveforms:') # get test set features 
features_test = get_features(X_test, features_test, sample_rate)

print(f'\n\nFeatures set: {len(features_train)+len(features_test)+len(features_valid)} total, {len(features_train)} train, {len(features_valid)} validation, {len(features_test)} test samples')
print(f'Features (MFC coefficient matrix) shape: {len(features_train[0])} mel frequency coefficients x {len(features_train[0][1])} time steps')

X_train = np.expand_dims(features_train,1)
X_valid = np.expand_dims(features_valid, 1)
X_test = np.expand_dims(features_test,1)

'''# convert emotion labels from list back to numpy arrays for PyTorch to work with '''
y_train = np.array(y_train)
y_valid = np.array(y_valid)
y_test = np.array(y_test)

del features_train, features_valid, features_test, waveforms

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

'''#### Scale the training data ####
# store shape so we can transform it back'''
N,C,H,W = X_train.shape

'''# Reshape to 1D because StandardScaler operates on a 1D array
# tell numpy to infer shape of 1D array with '-1' argument'''
X_train = np.reshape(X_train, (N,-1)) 
X_train = scaler.fit_transform(X_train)

'''# Transform back to NxCxHxW 4D tensor format'''
X_train = np.reshape(X_train, (N,C,H,W))

'''##### Scale the validation set ####'''
N,C,H,W = X_valid.shape
X_valid = np.reshape(X_valid, (N,-1))
X_valid = scaler.transform(X_valid)
X_valid = np.reshape(X_valid, (N,C,H,W))

'''#### Scale the test set ####'''
N,C,H,W = X_test.shape
X_test = np.reshape(X_test, (N,-1))
X_test = scaler.transform(X_test)
X_test = np.reshape(X_test, (N,C,H,W))

class parallel_all_you_want(nn.Module):
    '''# Define all layers present in the network'''
    def __init__(self,num_emotions):
        super().__init__() 
        
        '''################ TRANSFORMER BLOCK #############################
        # maxpool the input feature map/tensor to the transformer 
        # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor'''
        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])
        
        '''# define single transformer encoder layer
        # self-attention + feedforward network from "Attention is All You Need" paper
        # 4 multi-head self-attention layers each with 64-->512--->64 feedforward network'''
        transformer_layer = nn.TransformerEncoderLayer(
            d_model=40, # input feature (frequency) dim after maxpooling 128*563 -> 64*140 (freq*time)
            nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block
            dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 64-->512--->64
            dropout=0.4, 
            activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time
        )
        
        '''# I'm using 4 instead of the 6 identical stacked encoder layrs used in Attention is All You Need paper
        # Complete transformer block contains 4 full transformer encoder layers (each w/ multihead self-attention+feedforward)'''
        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)

        '''############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############
        # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)'''
        self.conv2Dblock1 = nn.Sequential(
            
            '''# 1st 2D convolution layer'''
            nn.Conv2d(
                in_channels=1, # input volume depth == input channel dim == 1
                out_channels=16, # expand output feature map volume's depth to 16
                kernel_size=3, # typical 3*3 stride 1 kernel
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(16), # batch normalize the output feature map before activation
            nn.ReLU(), # feature map --> activation map
            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size
            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training
             
             '''# 2nd 2D convolution layer identical to last except output dim, maxpool kernel'''
            nn.Conv2d(
                in_channels=16, 
                out_channels=32, # expand output feature map volume's depth to 32
                kernel_size=3,
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters
            nn.Dropout(p=0.3), 
            
            '''# 3rd 2D convolution layer identical to last except output dim'''
            nn.Conv2d(
                in_channels=32,
                out_channels=64, # expand output feature map volume's depth to 64
                kernel_size=3,
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=4, stride=4),
            nn.Dropout(p=0.3),)
        
        '''############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############
        # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)'''
        self.conv2Dblock2 = nn.Sequential(
            
            '''# 1st 2D convolution layer'''
            nn.Conv2d(
                in_channels=1, # input volume depth == input channel dim == 1
                out_channels=16, # expand output feature map volume's depth to 16
                kernel_size=3, # typical 3*3 stride 1 kernel
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(16), # batch normalize the output feature map before activation
            nn.ReLU(), # feature map --> activation map
            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size
            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training
             
             '''# 2nd 2D convolution layer identical to last except output dim, maxpool kernel'''
            nn.Conv2d(
                in_channels=16, 
                out_channels=32, # expand output feature map volume's depth to 32
                kernel_size=3,
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters
            nn.Dropout(p=0.3), 
            
            '''# 3rd 2D convolution layer identical to last except output dim'''
            nn.Conv2d(
                in_channels=32,
                out_channels=64, # expand output feature map volume's depth to 64
                kernel_size=3,
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=4, stride=4),
            nn.Dropout(p=0.3),
        )
        self.fc1_linear = nn.Linear(512*2+40,num_emotions) 
        
        '''### Softmax layer for the 8 output logits from final FC linear layer '''
        self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding
    def forward(self,x):
        
        '''############ 1st parallel Conv2D block: 4 Convolutional layers ############################
        # create final feature embedding from 1st convolutional layer'''

        '''# input features pased through 4 sequential 2D convolutional layers'''
        conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time
        
        '''# flatten final 64*1*4 feature map from convolutional layers to length 256 1D array 
        # skip the 1st (N/batch) dimension when flattening'''
        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) 
        
        '''############ 2nd parallel Conv2D block: 4 Convolutional layers #############################
        # create final feature embedding from 2nd convolutional layer 
        # input features pased through 4 sequential 2D convolutional layers'''
        conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time
        
        '''# flatten final 64*1*4 feature map from convolutional layers to length 256 1D array 
        # skip the 1st (N/batch) dimension when flattening'''
        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) 
        x_maxpool = self.transformer_maxpool(x)

        '''# remove channel dim: 1*40*70 --> 40*70'''
        x_maxpool_reduced = torch.squeeze(x_maxpool,1)
        
        '''# convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format
        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)'''
        x = x_maxpool_reduced.permute(2,0,1) 
        
        '''# finally, pass reduced input feature map x into transformer encoder layers'''
        transformer_output = self.transformer_encoder(x)
        
        '''# create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)
        # transformer outputs 64*140 (freq embedding*time) feature map, take mean of all columns i.e. take time average'''
        transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40
        
        '''############# concatenate freq embeddings from convolutional and transformer blocks ######
        # concatenate embedding tensors output by parallel 2*conv and 1*transformer blocks'''
        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)  

        '''######### final FC linear layer, need logits for loss #########################'''
        output_logits = self.fc1_linear(complete_embedding)  
        
        '''######### Final Softmax layer: use logits from FC linear, get softmax for prediction ######'''
        output_softmax = self.softmax_out(output_logits)
        
        '''# need output logits to compute cross entropy loss, need softmax probabilities to predict class'''
        return output_logits, output_softmax

device = 'cuda'

'''# instantiate model for 8 emotions and move to CPU for summary'''
model = parallel_all_you_want(len(emotions_dict)).to(device)

def criterion(predictions, targets): 
    return nn.CrossEntropyLoss()(input=predictions, target=targets)
optimizer = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)

def make_train_step(model, criterion, optimizer):
    
    '''# define the training step of the training phase'''
    def train_step(X,Y):
        
        '''# forward pass'''
        output_logits, output_softmax = model(X)
        predictions = torch.argmax(output_softmax,dim=1)
        
        accuracy = torch.sum(Y==predictions)/float(len(Y))
        
        '''# compute loss on logits because nn.CrossEntropyLoss implements log softmax'''
        loss = criterion(output_logits, Y) 
        
        '''# compute gradients for the optimizer to use '''
        loss.backward()
        
        '''# update network parameters based on gradient stored (by calling loss.backward())'''
        optimizer.step()
        
        '''# zero out gradients for next pass
        # pytorch accumulates gradients from backwards passes (convenient for RNNs)'''
        optimizer.zero_grad() 
        
        return loss.item(), accuracy*100
    return train_step

def validate(X,Y):
        
        '''# don't want to update any network parameters on validation passes: don't need gradient
        # wrap in torch.no_grad to save memory and compute in validation phase: '''
        with torch.no_grad(): 
            
            '''# set model to validation phase i.e. turn off dropout and batchnorm layers '''
            model.eval()
      
            '''# get the model's predictions on the validation set'''
            output_logits, output_softmax = model(X)
            predictions = torch.argmax(output_softmax,dim=1)
            
            '''# calculate the mean accuracy over the entire validation set'''
            accuracy = torch.sum(Y==predictions)/float(len(Y))
          
          '''# compute error from logits (nn.crossentropy implements softmax)'''
            loss = criterion(output_logits,Y)
            
        return loss.item(), accuracy*100

train_size = X_train.shape[0]

'''# pick minibatch size (of 32... always)'''
minibatch = 32

'''# instantiate model and move to GPU for training'''
model = parallel_all_you_want(num_emotions=len(emotions_dict)).to(device) 
print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )

'''# encountered bugs in google colab only, unless I explicitly defined optimizer in this cell...'''
optimizer = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)

'''# instantiate the training step function '''
train_step = make_train_step(model, criterion, optimizer=optimizer)

'''# instantiate the validation loop function'''
validate = make_validate_fnc(model,criterion)

train_losses=[]
valid_losses = []

'''# create training loop for one complete epoch (entire training set)'''
def train(optimizer, model, num_epochs, X_train, Y_train, X_valid, Y_valid):

    for epoch in range(num_epochs):
        
        '''# set model to train phase'''
        model.train()         
        
        '''# shuffle entire training set in each epoch to randomize minibatch order'''
        
        '''# instantiate scalar values to keep track of progress after each epoch so we can stop training when appropriate '''
        epoch_acc = 0 
        epoch_loss = 0
        num_iterations = int(train_size / minibatch)
        for i in tqdm(range(num_iterations)):
            
            '''# we have to track and update minibatch position for the current minibatch
            # if we take a random batch position from a set, we almost certainly will skip some of the data in that set
            # track minibatch position based on iteration number:'''
            batch_start = i * minibatch 
            
            '''# ensure we don't go out of the bounds of our training set:'''
            batch_end = min(batch_start + minibatch, train_size) 
            
            '''# ensure we don't have an index error'''
            actual_batch_size = batch_end-batch_start 
            
            '''# get training minibatch with all channnels and 2D feature dims'''
            X = X_train[batch_start:batch_end,:,:,:] 
            
            '''# get training minibatch labels'''
            Y = Y_train[batch_start:batch_end] 
            
            '''# instantiate training tensors'''
            X_tensor = torch.tensor(X, device=device).float() 
            Y=Y.astype(np.float32)
            
            Y_tensor = torch.tensor(Y,dtype=torch.long).cuda()
            
            '''# Pass input tensors thru 1 training step (fwd+backwards pass)'''
            loss, acc = train_step(X_tensor,Y_tensor) 
            
            '''# aggregate batch accuracy to measure progress of entire epoch'''
            epoch_acc += acc * actual_batch_size / train_size
            epoch_loss += loss * actual_batch_size / train_size
      
        '''# accumulate scalar performance metrics at each epoch to track and plot later'''
        train_losses.append(epoch_loss)
        
        '''# valid_losses.append(valid_loss)'''
        print(f'\nEpoch {epoch} --- loss:{epoch_loss:.3f}, Epoch accuracy:{epoch_acc:.2f}%')

train(optimizer, model, 200, X_train, y_train, X_valid, y_valid)

X_test_tensor = torch.tensor(X_test,device=device).float()

'''# Convert 4D test label set array to tensor and move to GPU'''
y_test_tensor = torch.tensor(y_test,dtype=torch.long,device=device)
X_valid_tensor = torch.tensor(X_valid,device=device).float()
Y_valid=y_valid.astype(np.float32)
Y_valid_tensor = torch.tensor(Y_valid[0:143],dtype=torch.long).cuda()

'''# Get the model's performance metrics using the validation function we defined'''
test_loss, test_acc = validate(X_valid_tensor,Y_valid_tensor)

print(f'Test accuracy is {test_acc:.2f}%')

