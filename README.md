# speech-emotion-recognition-using-transformers

#run speech-emotion-recognition(1).ipynb file ..it is the original code all the remaining codes are just trails on different augmentations

dataset used is RAVDESS Emotional speech audio ref: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio 

look the colab file set the path accordingly and run the code 

![image](https://github.com/akash1akash1/speech-emotion-recognition/assets/128292061/beecb009-6720-44c6-9e08-abf8b2ab4cfb)


all the information about the project results and analysis is clearly mentioned in the report.pdf section 

#results 
![image](https://github.com/akash1akash1/speech-emotion-recognition/assets/128292061/e49b645d-f443-4985-a19f-3b3029fa29b6)

Validation accuracy achieved is 74.22% 
#confusion matrix 
![image](https://github.com/akash1akash1/speech-emotion-recognition/assets/128292061/769c136c-23d0-4d67-9bbd-bdb1c3b88c66)




